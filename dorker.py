import requests, re, sys
from bs4 import BeautifulSoup
from functools import partial
from multiprocessing import Pool
import urllib
import urllib2
import tqdm
import re

def get_google(search_string, start):
    try:
        temp = []
        url = 'http://google.com/search'
        payload = {'q': search_string, 'start': start}
        my_headers = {'User-agent': 'Mozilla/11.0'}
        r = requests.get(url, params=payload, headers=my_headers)
        soup = BeautifulSoup(r.text, 'html.parser')
        h3tags = soup.find_all('h3', class_='r')
        for h3 in h3tags:
            try:
                temp.append(re.search('url\?q=(.+?)\&sa', h3.a['href']).group(1))
            except:
                continue
        return temp
    except:
        return temp


def get_bing(search_string, start):
    try:
        temp = []
        url = 'http://www.bing.com/search'
        payload = {'q': search_string, 'first': start}
        my_headers = {'User-agent': 'Mozilla/11.0'}
        r = requests.get(url, params=payload, headers=my_headers)
        results = re.findall('<h2>*?<a\s+[^>]*?href="([^"]*)', r.text)
        for url in results:
            temp.append(str(url))
        return temp
    except:
        return temp

def get_yahoo(search_string, start):
    try:
        temp = []
        url = "https://search.yahoo.com/search?p=" + search_string + "&b=" + str(start)
        my_headers = {'User-agent': 'Mozilla/11.0'}
        request = urllib2.Request(url, None, {'User-Agent':'Mosilla/5.0 (Macintosh; Intel Mac OS X 10_7_4) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.57 Safari/536.11'})
        urlfile = urllib2.urlopen(request)
        page = urlfile.read()
        soup = BeautifulSoup(page, "lxml")
        headers = soup.findAll('h3', {"class": "title"})
        for header in headers:
            if not header.a:
                continue
            u = header.a.get('href')
            temp.append(str(u))
        return temp
    except:
        return temp



def get_baidu(search_string, start):
    try:
        temp = []
        url = "http://www.baidu.com/s?wd=" + search_string+"&pn="+str(start)
        try:
            htmlpage = urllib2.urlopen(url,timeout = 15).read()
        except:
            pass
        soup = BeautifulSoup(htmlpage,'lxml')
        for child in soup.findAll("h3", {"class": "t"}):
            script = requests.get(child.a.get('href'),timeout = 15).content
            soup = BeautifulSoup(script, 'lxml')
            for child in soup.findAll("meta"):
                if "http" in str(child):
                    try:
                        url = str(child).split("'")[1]
                        temp.append(url)
                    except:
                        pass
        return temp
    except:
        return temp



def dork_scanner(search, pages, processes, searchengine):
    result = []
    search = search
    pages = pages
    processes = int(processes)
    if searchengine == 0:
        make_request = partial(get_google, search)
    elif searchengine == 1:
        make_request = partial(get_bing, search)
    elif searchengine == 2:
        make_request = partial(get_baidu, search)
    elif searchengine == 3:
        make_request = partial(get_yahoo, search)
    pagelist = [str(x * 10) for x in range(int(pages))]
    p = Pool(processes=processes)

    for tmp in tqdm.tqdm(p.imap_unordered(make_request, pagelist), total=len(pagelist)):
        result.extend(tmp)


    result = list(set(result))
    return result